<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tairan He</title>
  
  <meta name="author" content="Tairan He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--   <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tairan He 「何泰然」</name>
              </p>
              <p>I am a first-year Ph.D. student at the <a href="https://www.ri.cmu.edu">Robotics Institute</a> at <a href="https://www.cmu.edu">Carnegie Mellon University</a>, advised by <a href="http://www.gshi.me"> Guanya Shi</a> and <a href="https://www.cs.cmu.edu/~cliu6/"> Changliu Liu</a>.
              </p>
              <p>Previously, I received my Bachelor's degree in computer science at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a>, advised by <a href="http://wnzhang.net"> Weinan Zhang</a>. I also spent time at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research Asia</a>.
              </p>
              <p>Email: tairanh [AT] andrew.cmu.edu
              </p>
<!--               <p style="text-align:center"><font color="red"><em><strong>I will apply for PhD programs starting in 2023 Fall.</strong></em></font>
              </p> -->
              
<!--               <p>
                At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
              </p> -->
              <br>
<!--               <p style="text-align:center">
                  I'm looking for summer research opportunities in 2021!
              </p> -->
              <p style="text-align:center">
                <!-- <a href="mailto:tairanhe1999@gmail.com">Email</a> &nbsp/&nbsp -->
                <a href="data/TairanHe_CV_20230816.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=TVWH2U8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/TairanHe">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/TairanHe99">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/tairan-he-41a904294/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://space.bilibili.com/14145636">Bilibili</a> 
<!--                 <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
<!--                 <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a>  --><!-- &nbsp/&nbsp -->
<!--                 <a href="https://twitter.com/jon_barron">Twitter</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:26%;max-width:26%">
              <a href="images/TairanHe-2023_1M.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/TairanHe-2023_1M.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Topics</heading>
              <p>
                As a robotics researcher, my goal is to challenge conventional notions of what robots can achieve. I strive to push the limits of robotics by conducting research at the intersection of <u>machine learning</u> and <u>control theory</u>. My focus is on developing intelligent robots that possess <u>agility, stability, safety, and robustness</u>.
                <br>
                <br>
                To accomplish this, I explore ways to <u>bridge the gap between learning and control</u>, thereby creating a unified framework for robotics. Through my research, I aim to expand the capabilities of robots and transform how they can be used in a variety of fields. Ultimately, my goal is to change people's perceptions of robots and demonstrate the true potential of this exciting technology.

<!--                 Currently, I concetrate on the data-driven RL tasks (e.g., <b>imitation learning</b> and <b>batch reinforcement learning</b>) and <b>representation learning</b> in RL.  -->

<!--                 I keep reading an inspiring paper every day, and share my understanding and notes in the  <a href="https://www.zhihu.com/column/c_1300374076980932608"> Zhihu column</a>. -->
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              (* equal contribution)
            </td>
          </tr>
          <tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.08602" id="SafeDPA">
                <papertitle>Safe Deep Policy Adaptation</papertitle>
              </a>
              <br>
              Wenli Xiao*, <b>Tairan He*</b>, John Dolan, Guanya Shi 
              <br>
              Under review, &nbsp;2023&nbsp;
              <br>
              <em>Conference on Robot Learning (CoRL)</em>,&nbsp;2023 Workshop:&nbsp; Towards Reliable and Deployable Learning-Based Robotic Systems
              <br>
              <a href="https://arxiv.org/abs/2310.08602">ArXiv</a>
              &nbsp;/&nbsp;
              <a href="https://sites.google.com/view/safe-deep-policy-adaptation">Blog</a>
              &nbsp;/&nbsp;
              <a href="https://www.youtube.com/watch?v=PkyRzlRQVbE">Video</a>
              <br>
              <font color='gray'>A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments.</font>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.03379" id="ACS">
                <papertitle>Progressive Adaptive Chance-Constrained Safeguards for Reinforcement Learning</papertitle>
              </a>
              <br>
              Zhaorun Chen, Binhao Chen, <b>Tairan He</b>, Liang Gong, Chengliang Liu
              <br>
              Under review, &nbsp;2023&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2310.03379">ArXiv</a>
              <br>
              <font color='gray'>Safety assurance of Reinforcement Learning (RL) is critical for exploration in real-world scenarios. In handling the Constrained Markov Decision Process, current approaches experience intrinsic difficulties in trading-off between optimality and feasibility. Direct optimization methods cannot strictly guarantee state-wise in-training safety while projection-based methods are usually inefficient and correct actions through lengthy iterations. To address these two challenges, this paper proposes an adaptive surrogate chance constraint for the safety cost, and a hierarchical architecture that corrects actions produced by the upper policy layer via a fast Quasi-Newton method. Theoretical analysis indicates that the relaxed probabilistic constraint can sufficiently guarantee forward invariance to the safe set. We validate the proposed method on 4 simulated and real-world safety-critical robotic tasks. Results indicate that the proposed method can efficiently enforce safety (nearly zero-violation), while preserving optimality (+23.8%), robustness and generalizability to stochastic real-world settings.</font>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2302.03122" id="SafeRL-Survey">
                <papertitle>State-wise Safe Reinforcement Learning: A Survey</papertitle>
              </a>
              <br>
              Weiye Zhao, <b>Tairan He</b>, Rui Chen, Tianhao Wei, Changliu Liu
              <br>
              <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, &nbsp;2023&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2302.03122">ArXiv</a>
              <br>
              <font color='gray'>Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potential future directions.</font>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=OnM3R47KIiU" id="PatchAIL">
                <papertitle>Visual Imitation Learning with Patch Rewards</papertitle>
              </a>
              <br>
              Minghuan Liu, <b>Tairan He</b>, Weinan Zhang, Shuicheng Yan, Zhongwen Xu
<!--               <br> -->
<!--               <em>Conference on Neural Information Processing Systems (NeurIPS)</em>,&nbsp;2022&nbsp; -->
<!--               Conference on Neural Information Processing Systems (NeurIPS) DeepRL Workshop, 2022  -->
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, &nbsp;2023&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2302.00965">ArXiv</a>
              &nbsp;/&nbsp;
              <a href="https://sites.google.com/view/patchail/">Blog</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/sail-sg/PatchAIL">Code</a>
<!--               &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a> -->
              <!-- <font color="red"><em><strong>(oral)</strong></em></font> -->
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <font color='gray'>Explainable visual imitation learning from high-dimensional image demonstrations remains a grand challenge. The problem raises issues in many aspects, such as representation learning, sample complexity, and training stability. Prior works mostly neglect the dense information provided in the image demonstration. This paper proposes an efficient visual imitation learning method, PatchAIL, to learn explainable patch-based rewards that measure the expertise of different local parts of given images. The patch-based knowledge is used to regularize the aggregated reward and stabilize the training. We evaluate our method on the standard pixel-based benchmark DeepMind Control Suite in experiments. The empirical results confirm that PatchAIL supports efficient training that outperforms baseline methods and provides valuable interpretations for visual imitation learning.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2209.09134" id="SOS-SI">
                <papertitle>Safety Index Synthesis via Sum-of-Squares Programming</papertitle>
              </a>
              <br>
              Weiye Zhao*, <b>Tairan He*</b>, Tianhao Wei, Simin Liu, Changliu Liu
              <br>
<!--               <em>Conference on Neural Information Processing Systems (NeurIPS)</em>,&nbsp;2022&nbsp; -->
              <em>American Control Conference (ACC)</em>, &nbsp;2023&nbsp;
<!--               <br>
              <a href="https://github.com/TairanHe/ISSA">Code</a> -->
<!--               &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a> -->
              <!-- <font color="red"><em><strong>(oral)</strong></em></font> -->
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <a href="https://arxiv.org/abs/2209.09134">ArXiv</a>
<!--               &nbsp;/&nbsp;<a href="https://github.com/apexrl/EBIL-torch">Code</a> -->
              <br>
              <font color='gray'>Control systems often need to satisfy strict safety requirements. Safety index provides a handy way to evaluate the safety level of the system and derive the resulting safe control policies. However, designing safety index functions under control limits is difficult and requires a great amount of expert knowledge. This paper proposes a framework for synthesizing the safety index for general control systems using sum-of-squares programming. Our approach is to show that ensuring the non-emptiness of safe control on the safe set boundary is equivalent to a local manifold positiveness problem. We then prove that this problem is equivalent to sum-of-squares programming via the Positivstellensatz of algebraic geometry. We validate the proposed method on robot arms with different degrees of freedom and ground vehicles. The results show that the synthesized safety index guarantees safety and our method is effective even in high-dimensional robot systems.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.01041" id="UAISSA">
                <papertitle>Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models</papertitle>
              </a>
              <br>
              Weiye Zhao*, <b>Tairan He*</b>, Changliu Liu
              <br>
              <em>Learning for Dynamics & Control Conference (L4DC)</em>, &nbsp;2023&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2210.01041">ArXiv</a>
<!--               &nbsp;/&nbsp;<a href="https://github.com/apexrl/EBIL-torch">Code</a> -->
              <br>
              <font color='gray'>Safety is one of the biggest concerns to applying reinforcement learning (RL) to the physical world. In its core part, it is challenging to ensure RL agents persistently satisfy a hard state constraint without white-box or black-box dynamics models. This paper presents an integrated model learning and safe control framework to safeguard any agent, where its  dynamics are learned as Gaussian processes. The proposed theory provides (i) a novel method to construct an offline dataset for model learning that best achieves safety requirements; (ii) a parameterization rule for safety index to ensure the existence of safe control; (iii) a safety guarantee in terms of probabilistic forward invariance when the model is learned using the aforementioned dataset. Simulation results show that our framework guarantees almost zero safety violation on various continuous control tasks.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">              
              <!-- <a href="under-review.html" id="AutoCost"> -->
              <a href="https://arxiv.org/abs/2301.10339" id="AutoCost">
                <papertitle>AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning</papertitle>
              </a>
              <br>
              <b>Tairan He</b>, Weiye Zhao, Changliu Liu
              <br>
              <em>AAAI Conference on Artificial Intelligence (AAAI)</em>,&nbsp;2023&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2301.10339">ArXiv</a>
<!--           Under Review, &nbsp;2022&nbsp; -->
<!--               <br>
              <a href="https://github.com/TairanHe/ISSA">Code</a> -->
<!--               &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a> -->
              <!-- <font color="red"><em><strong>(oral)</strong></em></font> -->
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <font color='gray'>Safety is a critical hurdle that limits the application of deep reinforcement learning (RL) to real-world control tasks. To this end, constrained reinforcement learning leverages cost functions to improve safety in constrained Markov decision processes. However, such constrained RL methods fail to achieve zero violation even when the cost limit is zero. This paper analyzes the reason for such failure, which suggests that a proper cost function plays an important role in constrained RL. Inspired by the analysis, we propose AutoCost, a simple yet effective framework that automatically searches for cost functions that help constrained RL to achieve zero-violation performance. We validate the proposed method and the searched cost function on the safe RL benchmark Safety Gym. We compare the performance of augmented agents that use our cost function to provide additive intrinsic costs with baseline agents that use the same policy learners but with only extrinsic costs. Results show that the converged policies with intrinsic costs in all environments achieve zero constraint violation and comparable performance with baselines.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
<!--               <a href="https://openreview.net/forum?id=UGp6FDaxB0f" id="A2lS">
                <papertitle>Reinforcement Learning with Automated Auxiliary Loss Search</papertitle>
              </a> -->
              <a href="https://openreview.net/forum?id=_3XVbh6L2c" id="A2LS">
                <papertitle>Reinforcement Learning with Automated Auxiliary Loss Search</papertitle>
              </a>
              <br>
              <b>Tairan He</b>, Yuge Zhang, Kan Ren, Minghuan Liu, Che Wang, Weinan Zhang, Yuqing Yang, Dongsheng Li
              <br>
              <em>Conference on Neural Information Processing Systems (NeurIPS)</em>,&nbsp;2022&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2210.06041">ArXiv</a>
              &nbsp;/&nbsp;
              <a href="https://seqml.github.io/a2ls/">Blog</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/microsoft/autorl-research/tree/main/a2ls">Code</a>
<!--               <br>
              <a href="https://github.com/TairanHe/ISSA">Code</a> -->
<!--               &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a> -->
              <!-- <font color="red"><em><strong>(oral)</strong></em></font> -->
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <font color='gray'>A good state representation is crucial to solving complicated reinforcement learning (RL) challenges. Many recent works focus on designing auxiliary losses for learning informative representations. Unfortunately, these handcrafted objectives rely heavily on expert knowledge and may be sub-optimal. In this paper, we propose a principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing auxiliary loss functions for RL. Specifically, based on the collected trajectory data, we define a general auxiliary loss space of size 7.5×10^20 and explore the space with an efficient evolutionary search strategy. Empirical results show that the discovered auxiliary loss (namely, A2-winner) significantly improves the performance on both high-dimensional (image) and low-dimensional (vector) unseen tasks with much higher efficiency, showing promising generalization ability to different settings and even different benchmark domains. We conduct a statistical analysis to reveal the relations between patterns of auxiliary losses and RL performance. </font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://proceedings.mlr.press/v164/zhao22a.html" id="ISSA">
                <papertitle>Model-free Safe Control for Zero-Violation Reinforcement Learning</papertitle>
              </a>
              <br>
              Weiye Zhao, <b>Tairan He</b>, Changliu Liu
              <br>
              <em>Conference on Robot Learning (CoRL)</em>,&nbsp;2021&nbsp;
              <br>
              <a href="https://github.com/TairanHe/ISSA">Code</a>
<!--               &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a> -->
              <!-- <font color="red"><em><strong>(oral)</strong></em></font> -->
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <font color='gray'>We present a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains 95% ± 9 % cumulative reward compared to state-of-the-art safe DRL methods.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.09395" id="EBIL">
                <papertitle>Energy-Based Imitation Learning</papertitle>
              </a>
              <br>
              Minghuan Liu, <b>Tairan He</b>, Minkai Xu, Weinan Zhang 
              <br>
              <em>International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</em>
              ,&nbsp;2021&nbsp;
              <font color="red"><em><strong>(oral)</strong></em></font>
              <br>
              <a href="https://arxiv.org/abs/2004.09395">ArXiv</a>&nbsp;/&nbsp;<a href="https://github.com/apexrl/EBIL-torch">Code</a>
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <br>
              <font color='gray'>We propose EBIL, a two-step solution for imitation learning: first estimate the energy of expert’s occupancy measure, and then take the energy to construct a surrogate reward function as a guidance for the agent to learn the desired policy. EBIL combines the idea of both EBM and occupancy measure matching, and via theoretic analysis we reveal that EBIL and Max-Entropy IRL (MaxEnt IRL) approaches are two sides of the same coin, and thus EBIL could be an alternative of adversarial IRL methods. Extensive experiments on qualitative and quantitative evaluations indicate that EBIL is able to recover meaningful and interpretative reward signals while achieving effective and comparable performance against existing algorithms on IL benchmarks.</font>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
          <tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:middle"> 
              <!-- padding 上 右 下 左 -->
              <a href="http://wukefenggao.cn" id="wkfg">
                <papertitle>SJTU Anonymous Forum 「无可奉告」</papertitle>
              </a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 0px 20px;width:25%;vertical-align:middle"><img src="images/wkfgicon.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <br>
              <a href="https://github.com/TairanHe/SJTU-Anonymous_Forum"> Android Code</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/oscardhc/Forum"> iOS Code</a>
              &nbsp;/&nbsp;
              <a href="http://wukefenggao.cn"> Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://www.bilibili.com/video/BV1Rp4y187ZJ"> Farewell Video</a>
              <br>
              <p>A carefree forum platform for SJTUers sharing and talking with anonymous identity.</p>
              <p>More than <font color="red"><em><strong>10000+</strong></em></font> users used「无可奉告」in the SJTU campus.</p>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
            </td>
          </tr>
          <tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <b>Carnegie Mellon University</b>, Pittsburgh, PA, USA
              <br>
              Ph.D. in Robotics • Aug. 2023 to Present
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/cmu-seal-r.png" width="75" height="75"></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <b>Shanghai Jiao Tong University</b>, Shanghai, China
              <br>
              B.E. in Computer Science • Aug. 2018 to Jun. 2023
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/sjtu-logo.png" width="75" height="75"></td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experiences</heading>
            </td>
          </tr>
          <tr>
        </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="http://icontrol.ri.cmu.edu"> Intelligent Control Lab</a>, <b>Carnegie Mellon University</b>
              <br>
              Research Intern • Jan. 2022 to Jan. 2023
              <br>
              Advisor: <a href="https://www.cs.cmu.edu/~cliu6/"> Changliu Liu</a>
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/cmu-seal-r.png" width="75" height="75"></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research</a>
              <br>
              Research Intern • March. 2021 to Dec. 2021
              <br>
              Advisor: <a href="https://scholar.google.com/citations?user=kCQdkrQAAAAJ&hl=en"> Yuge Zhang </a> and <a href="https://www.saying.ren"> Kan Ren</a>
              </br>
            </td>
            <td style="padding:0px 24px 0px 20px;width:10%;vertical-align:middle"><img src="images/msft-logo.png" width="65" height="65"></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://apex.sjtu.edu.cn"> APEX Lab</a>, <b>Shanghai Jiao Tong University</b>
              <br>
              Research Intern • Jul. 2019 to Jan. 2023
              <br>
              Advisor: <a href="http://wnzhang.net"> Weinan Zhang</a>
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/sjtu-logo.png" width="75" height="75"></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Reviewer Service</heading>
              <p>
              International Conference on Learning Representations <b>(ICLR)</b>, 2024
              <br>
              IEEE Conference on Decision and Control <b>(CDC)</b>, 2023
              <br>  
              Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2023
              <br>
              Learning for Dynamics & Control Conference <b>(L4DC)</b> 2023
              <br>
              AAAI Conference on Artificial Intelligence <b>(AAAI)</b> 2023, 2024
              <br>
              Conference on Robot Learning <b>(CoRL)</b> 2022, 2023
              </p>
            </td>
          </tr>
        </tbody></table>


<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="http://wukefenggao.cn" id="wkfg">
                <papertitle>SJTU Anonymous Forum 「无可奉告」</papertitle>
              </a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/wkfgicon.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
        </tbody></table> -->




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <br>
                                    <div>
                                        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=Biz007_Pw8FVsAWycLRoKM_5XR_da9ccb8qGNbWVwnk&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
                                        <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">
 -->                               </div>
                                </td>
                            </tr>
                        </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px">
                <p font-size:small;="">
                    <br>
                    <br>
                    </p><div style="float:left;">
                        Updated at Oct. 2023
                    </div>
                    <div style="float:right;">
                        Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                    </div>
                    <br>
                    <br>        
                <p></p>                           
            </td>
          </tr>
        </tbody></table>



      </td>
    </tr>
  </table>
</body>

</html>
